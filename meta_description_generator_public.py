# -*- coding: utf-8 -*-
"""meta_description_generator_public.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mlmbl2pKydhIlhEU856i--y-QcTLF2GX
"""

#Data organization dependencies
from os import remove
import pandas as pd
from google.colab import data_table
data_table.enable_dataframe_formatter()

#Web scraping dependencies
!pip install beautifulsoup4
from bs4 import BeautifulSoup
from urllib.request import Request, urlopen
from urllib.parse import urljoin
!pip install requests
import requests
!pip install justext
import justext
import re

#NLP dependencies
!pip install transformers
from transformers import pipeline
# Summarize textual content using BART in pytorch
bart_summarizer = pipeline("summarization")

#Obtain list of links on a domain
domain = "https://www.barakatrestaurant.com"
req = Request(domain, headers={'User-Agent': 'Mozilla/5.0'})
html_page = urlopen(req)
soup = BeautifulSoup(html_page, "lxml")

links = []
for link in soup.findAll('a'):
    links.append(urljoin(domain, link.get('href')))

#Links cleanup for None types
links = [link for link in links if link is not None]  
#Links cleanup for non-URL objects
for link in links:
    if not link.startswith("http"):
        links.remove(link)
#Links cleanup for duplicate URLs
links = [link for n, link in enumerate(links) if link not in links[:n]] 
#Links cleanup for social URLs
socials = ["instagram", "facebook", "twitter", "linkedin", "tiktok", "google", "maps", "mealsy"]
clean_links = []
for link in links:
  if not any(social in link for social in socials):
    clean_links.append(link)

# Extract and clean text from each link 
content = []
for link in clean_links:
    response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})
    paragraphs = justext.justext(response.content, justext.get_stoplist("English"))
    for_processing = []
    block_words = ["cookies", "privacy", "policy", "instagram", "facebook", "twitter", "linkedin"]
    for paragraph in paragraphs:
        paragraph = paragraph.text
        if len(paragraph) < 50:
            continue
        elif not any(block in paragraph for block in block_words):
            for_processing.append(paragraph)
    for_processing = ' '.join(for_processing[:5])
    content.append(for_processing)

df = pd.DataFrame({'link': clean_links, 'content': content})

# Summarize content for meta descriptions
summarized_descriptions = []
for item in content:
    print(item)
    print(bart_summarizer(item, min_length = 20, max_length = 50))
    summarized_descriptions.append(bart_summarizer(item, min_length = 20, max_length = 50))

from pandas.core.internals.managers import T
from numpy.ma.core import count
#Retrieve values from list of dictionaries in summarized_descriptions
meta_descriptions = [summary[0]["summary_text"] for summary in summarized_descriptions]

# General data cleaning
meta_descriptions_clean1 = []
for description in meta_descriptions:
  #Clean up leading and trailing spaces:
  description = description.strip()
  #Clean up excessive spaces
  description = re.sub(' +', ' ', description)
  #Clean up punctuation spaces
  description = description.replace(' .', '.')
  #Clean up incomplete sentences
  if "." in description and not description.endswith("."):
    description = description.split(".")[:-1]
    description = description[0]
  meta_descriptions_clean1.append(description)

meta_descriptions_clean2 = []
#Add a period to all sentences (if missing)
for description in meta_descriptions_clean1:
  if not description.endswith("."):
    description = description + "."
  meta_descriptions_clean2.append(description)

#Adhere to meta description length of approx. 160 characters
#Find the index of the punctuation character desired
def find_all(string, character):
  index = string.find(character)
  while index != -1:
    yield index
    index = string.find(character, index + 1)

#Store truncation points
truncation_points = []
character = "."
for description in meta_descriptions_clean2:
  indexes = list(find_all(description, character))
  truncation_points.append(indexes)

# Auto truncate
meta_descriptions_clean3 = []
character = "."
for description in meta_descriptions_clean2:
  if len(description) > 160 and description.count(character) > 1:
    split = description.split(character)[:-2]
    description = character.join(split)
    if not description.endswith("."):
      description = description + "."
    meta_descriptions_clean3.append(description)
  else:
    meta_descriptions_clean3.append(description)
 
#Verify length adherence for non-truncated descriptions
len_meta_descriptions = []
for description in meta_descriptions_clean2:
  len_meta_descriptions.append(len(description))

#Verify length adherence for auto-truncated descriptions
truncated_len_meta_descriptions = []
for description in meta_descriptions_clean3:
  truncated_len_meta_descriptions.append(len(description))

# Organize results into dataframe    
df = pd.DataFrame({'link': clean_links, 'content': content, 'meta_descriptions': meta_descriptions_clean2, 'description_length': len_meta_descriptions, 'truncation_points': truncation_points, 'truncated_descriptions': meta_descriptions_clean3, 'truncated_length': truncated_len_meta_descriptions})
df